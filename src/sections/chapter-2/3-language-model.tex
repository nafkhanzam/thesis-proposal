\section{\textit{Language Model}}

\newcommand{\wordem}{\textit{word embedding}}
\newcommand{\Wordem}{\textit{Word embedding}}
\newcommand{\mwordem}{\textit{multilingual word embedding}}
\newcommand{\Mwordem}{\textit{Multilingual word embedding}}
\newcommand{\multil}{\textit{multilingual}}

Teks yang dapat dipahami oleh manusia tidak dapat dipahami secara langsung oleh model \gls{NLP} sehingga diperlukan sebuah bentuk representasi lain.
Ada beberapa teknik dalam membuat representasi dari sebuah teks, seperti \textit{bag of words}, \textit{n-gram}, \wordem{}, \gls{SRL}, dan \gls{AMR}.
Dari representasi-representasi tersebut, dapat dilakukan berbagai macam \textit{task} seperti peringkasan teks, klasifikasi sentimen, mesin translasi, \textit{question-answering}, deteksi parafrasa, dan lain-lain.

\Wordem{} merupakan representasi vektor berdimensi tinggi yang merepresentasikan kedudukan sebuah kata di antara kata-kata lain.
\Wordem{} dapat dibentuk dengan berbagai metode.
Ada metode yang tidak memberikan konteks pada kata seperti Word2Vec \citek{mikolov2013}, GloVe \citek{pennington2014}, dan FastText \citek{bojanowski2017}.
Metode tersebut menghasilkan yang disebut dengan \textit{non-contextual} \wordem{}.
Ada juga metode yang memberikan konteks pada kata dari sebuah kalimat, yang disebut dengan \textit{contextual} \wordem{}.
Dalam memahami konteks sebuah kata dari sebuah kalimat, diperlukan untuk memahami membaca keseluruhan kalimat tersebut.

\Glsfirst{RNN}, \glsfirst{LSTM}, dan \glsfirst{GRU} kerap digunakan sebagai pendekatan sequence modeling dan permasalahan transduction seperti language modeling dan machine translation.
Pendekatan tersebut berguna untuk memahami konteks sebuah kata pada suatu kalimat.
Transformer merupakan sebuah sequence-to-sequence model yang terdiri dari \textit{encoder} dan \textit{decoder}.
Transformer bergantung sepenuhnya terhadap mekanisme atensi untuk menggambarkan ketergantungan global antara input dan output.
Model berbasis transformer dapat berjalan secara paralel sehingga dapat mempercepat proses training.

Salah satu pengembangan \textit{encoder} dari model transformer adalah \glsfirst{BERT}.
\gls{BERT} merupakan teknik machine learning berbasis transformer untuk \gls{NLP} yang dikembangkan oleh Google \citek{devlin2019}.
\gls{BERT} memiliki kemampuan untuk memahami konteks dalam sebuah kalimat dan menggunakannya untuk menghasilkan hasil yang lebih akurat daripada model pembelajaran mesin sebelumnya.
\gls{BERT} merupakan pengembangan komponen \textit{encoder} pada model transformer.
\gls{BERT} dilatih terhadap dua \textit{task} yaitu \textit{language modeling} dan \textit{next sentence prediction}.
Sebagai hasil dari proses pembelajaran ini adalah, \gls{BERT} mempelajari embedding kontekstual untuk kata-kata yang ada.

Kemudian dikembangkan \glsfirst{BART} yang mampu melakukan denoising autoencoder yang memapping dokumen terkorupsi terhadap dokumen originalnya \citek{lewis2020}.
\gls{BART} diimplementasikan sebagai model berbasis transformer dengan bidirectional encoder dan left-to-right autoregressive decoder.
\gls{BART} memiliki arsitektur \textit{encoder} seperti \gls{BERT} \citek{devlin2019} dan \textit{decoder} seperti \gls{GPT2} \citek{radford2019}.
\gls{BART} menggunakan teknik pelatihan dengan mengacak urutan kalimat dan menggunakan \textit{masking} untuk diisi kata-kata yang hilang.
Model ini berguna sangat efektif setelah dilakukan \textit{fine-tuning} ke \textit{task} generasi teks maupun pemahaman teks.

% kecuali perbedaan-perbedaan berikut ini.
% \begin{enumerate}
%   \item Setiap layer dari decoder menampilkan cross-attention  pada layer tersembunyi terakhir dari encoder.
%   \item \gls{BERT} menggunakan tambahan feed-forward neural network sebelum memprediksi kata, \gls{BART} tidak.
% \end{enumerate}

Dalam menangani \textit{task} yang memerlukan lebih dari satu bahasa, seperti mesin translasi, digunakan representasi \mwordem{}.
Model \multil{} dapat dibangun dengan menambahkan \textit{vocabulary} dari bahasa lain dalam data \textit{training}.
\gls{BERT} juga dapat digunakan untuk menghasilkan \mwordem{}.
\textcite{devlin2019} melatih \gls{BERT} dengan menggunakan dataset yang terdiri dari 101 bahasa untuk menghasilkan \multil{} model bernama mBERT.
\gls{BART} juga dikembangkan untuk menghasilkan representasi \mwordem{} bernama mBART \citek{liu2020}.

Menurut \textcite{bi2020}, teknik \textit{pre-training} pada \gls{BERT} dan \gls{BART} kurang konsisten dengan tujuan beberapa \textit{task} generasi teks, seperti \textit{question answering}.
\textcite{bi2020} mengusulkan \glsfirst{PALM} dengan adanya skema yang menggabungkan \textit{pre-training} \textit{auto-encoding} dan \textit{autoregressive} \textit{language model} pada korpus besar tak berlabel.
\gls{PALM} didesain untuk mengenerasi teks baru yang bergantung pada konteks.
Skema ini menangani ketidakcocokan antara teknik \textit{pre-training} dan \textit{fine-tuning} yang ada sebelumnya.
Namun, model \gls{PALM} untuk \mwordem{} belum pernah dikembangkan.
