\section{Analisis Solusi}

Terdapat dua aspek dari permasalah keterbatasan dataset yang dapat diperbaiki, yaitu jumlah dan kualitas dataset.
Pada \amrparsing{}, dataset silver diperlukan untuk membangun dataset lintas bahasa dari dataset gold yang telah ada dalam Bahasa Inggris.
\textcite{putra2022} telah membangun dataset silver untuk Bahasa Indonesia dari dataset AMR 2.0 dan korpus paralel BPPT-PANL \citek{bppt2009}.
Jumlah dataset silver dapat diperbanyak dengan cara-cara berikut:
\begin{enumerate}
  \item Menggunakan dataset AMR 3.0.
  Dataset AMR 3.0 memiliki sekitar 20,000 pasangan kalimat dan graf \AMR{} lebih banyak dibandingkan dengan Dataset AMR 2.0.

  \item Menggunakan korpus paralel tambahan IWSLT17.
  \textcite{putra2022} hanya menggunakan korpus paralel BPPT-PANL \citek{bppt2009}.
  Korpus paralel IWSLT17 \citek{cettolo2017} dapat menambah sampai 107,329 pasang dataset silver.

  \item Mengadopsi teknik augmentasi data oleh \textcite{lee2022}.
  Melakukan \ti{parsing} graf \AMR{} dari dataset gold menjadi teks berbahasa Inggris, lalu dilakukan translasi ke Bahasa Indonesia.
  Teknik ini dapat menambah jumlah pasangan dataset silver menjadi sampai dua kali lipat.
\end{enumerate}

Peningkatan kualitas dataset silver dapat dilakukan dengan menggunakan teknik \ti{ensemble} oleh \textcite{lee2022}.
Berdasarkan hasil penelitian \textcite{lee2022}, algoritma graphene-Smatch \citek{hoang2021} dengan melakukan \ti{ensembling} 5 model \ti{state of the art} \AMR{} \ti{parser} dapat meningkatkan kualitas dataset silver secara signifikan.

Teknik \pretraining{} graf oleh \textcite{bai2022} yang menggunakan \ti{language model} \gls{BART} terbukti dapat meningkatkan skor \SMATCH{} untuk \amrparsing{}.
Teknik tersebut belum dicoba untuk \crosslingual{} \amrparsing{}.
Skema model pelatihan \crosslingual{} \amrparsing{} telah diusulkan oleh \textcite{blloshmi2020}, yaitu \ti{zero-shot}, \ti{language-specific}, dan \ti{bilingual}.
\cref{fig:2-xl-amr-training-schemas} menunjukkan tiga skema pelatihan tersebut.
Teknik \pretraining{} graf oleh \textcite{bai2022} tersebut dapat dilakukan dengan skema pelatihan \textcite{blloshmi2020} untuk membangun model \crosslingual{} untuk \ti{task} \amrparsing{}.

\fig[0.8]{2-xl-amr-training-schemas}
  {sections/chapter-3/2-xl-amr-training-schemas.png}
  {Model pelatihan \crosslingual{} untuk Bahasa Indonesia: (a) \ti{zero-shot}, (b)\ti{language-specific}, dan (c) \ti{bilingual} \citek{putra2022}.}

Teknik \ti{masked pre-training} \ti{language model} \gls{PALM} \citek{bi2020} dapat menghasilkan kinerja yang lebih baik dari \gls{MASS}, \gls{BART}, maupun T5 untuk \ti{task} \ti{question answering}, peringkasan teks, generasi pertanyaan, dan generasi respon.
\gls{PALM} dibangun dengan fokus untuk generasi teks penerus input yang diberikan.
\gls{PALM} diharapkan dapat meningkatkan kualitas \amrparsing{} dengan menggunakan teknik \ti{masking} oleh \textcite{bai2022}.
Namun untuk \amrparsing{} lintas bahasa diperlukan model \multil{} \gls{PALM}.
\Multil{} \gls{PALM} dapat dibangun dengan melanjutkan \pretraining{} dengan data baru berbahasa Indonesia.
Dataset tak berlabel berbahasa Indonesia yang digunakan oleh model mBART \citek{tang2020} dapat digunakan untuk melakukan \pretraining{} \multil{} \gls{PALM}.
